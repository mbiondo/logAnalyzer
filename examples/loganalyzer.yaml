# LogAnalyzer Configuration with Pipeline Architecture

# Persistence configuration - Write-Ahead Logging (WAL)
persistence:
  enabled: true                    # Enable persistence for log durability
  dir: "/data/wal"                # Directory for WAL files (mounted volume)
  max_file_size: 104857600        # 100MB per WAL file
  buffer_size: 100                # Buffer 100 logs before flush
  flush_interval: 5               # Flush every 5 seconds
  retention_hours: 24             # Keep WAL files for 24 hours
  sync_writes: false              # Async writes for better performance

# Output buffer configuration - Retry logic for failed outputs
output_buffer:
  enabled: true                    # Enable output buffering with retry
  dir: "/data/buffers"            # Directory for buffer files
  max_queue_size: 500             # 500 logs per output in memory
  max_retries: 5                  # Retry up to 5 times
  retry_interval: 10s             # Start with 10s, exponential backoff (capped at 2 minutes)
  max_retry_delay: 120s           # Max 2 minutes between retries
  flush_interval: 15s             # Persist retry queue every 15s
  dlq_enabled: true               # Enable Dead Letter Queue
  dlq_path: "/data/dlq"           # DLQ for permanently failed logs

inputs:
  # Docker container logs
  - type: docker
    name: "docker-demo"
    config:
      # Monitor specific containers
      container_filter: 
        - "loganalyzer-demo-app"
      stream: "stdout"
      # Resilient plugin configuration
      resilient: true              # Enable automatic reconnection (default: true)
      retry_interval: 10           # Retry every 10 seconds if connection fails
      max_retries: 0               # Infinite retries (0 = never give up)
      health_check_interval: 30    # Check health every 30 seconds

  # HTTP endpoint for external logs
  - type: http
    name: "http-api"
    config:
      port: "8080"
      resilient: true              # Enable automatic reconnection

  # Kafka topic consumption
  - type: kafka
    name: "kafka-logs"
    config:
      brokers:
        - "kafka:29092"
      topic: "application-logs"
      group_id: "loganalyzer-group"
      start_offset: "latest"
      min_bytes: 1
      max_bytes: 10485760  # 10MB
      # Resilient plugin configuration
      resilient: true              # Enable automatic reconnection
      retry_interval: 15           # Retry every 15 seconds if Kafka unavailable
      max_retries: 0               # Infinite retries
      health_check_interval: 30    # Check connection health every 30 seconds

outputs:
  # Pipeline 1: All logs to Elasticsearch with filtering
  - type: elasticsearch
    name: "elasticsearch-all"
    sources: []  # Accept from all inputs
    filters:
      # Filter by log level
      - type: level
        config:
          levels: ["INFO", "WARN", "ERROR"]
      
      # Filter by regex patterns (exclude DEBUG noise)
      - type: regex
        config:
          patterns: ["request id="]
          mode: "exclude"
          field: "message"
      
      # Rate limit to prevent Elasticsearch overload (5 logs/sec, burst of 20)
      - type: rate_limit
        config:
          rate: 5.0
          burst: 20
    config:
      addresses:
        - "http://elasticsearch:9200"
      index: "loganalyzer-{yyyy.MM.dd}"
      batch_size: 50
      timeout: 30
      # Resilient plugin configuration
      resilient: true              # Enable automatic reconnection (default: true)
      retry_interval: 10           # Retry every 10 seconds if ES unavailable
      max_retries: 0               # Infinite retries (0 = never give up)
      health_check_interval: 30    # Check ES health every 30 seconds

  # Pipeline 2: Only Docker logs to Prometheus metrics (no filtering)
  - type: prometheus
    name: "prometheus-metrics"
    sources: ["docker-demo"]  # Only from docker-demo input
    filters: []  # No filtering - all logs become metrics
    config:
      port: 9091

  # Pipeline 3: Errors to Console for debugging
  - type: console
    name: "console-errors"
    sources: []  # Accept from all inputs
    filters:
      - type: level
        config:
          levels: ["WARN", "ERROR"]
      # Rate limit console output (2 logs/sec, burst of 5)
      - type: rate_limit
        config:
          rate: 2.0
          burst: 5
    config:
      target: "stdout"
      format: "json"

  # Pipeline 4: JSON logs from HTTP to Elasticsearch with parsing
  - type: elasticsearch
    name: "elasticsearch-json"
    sources: ["http-api"]  # Only from HTTP input
    filters:
      # Parse JSON from message field
      - type: json
        config:
          field: "message"
          flatten: true
          ignore_errors: false
      # Filter by parsed level field
      - type: level
        config:
          levels: ["INFO", "WARN", "ERROR"]
    config:
      addresses:
        - "http://elasticsearch:9200"
      index: "json-logs-{yyyy.MM.dd}"
      batch_size: 50
      timeout: 30
      # Resilient plugin configuration
      resilient: true
      retry_interval: 10
      max_retries: 0
      health_check_interval: 30

  # Pipeline 5: Kafka logs to Elasticsearch with parsing
  - type: elasticsearch
    name: "elasticsearch-kafka"
    sources: ["kafka-logs"]  # Only from Kafka input
    filters:
      # Parse JSON from message field
      - type: json
        config:
          field: "message"
          flatten: true
          ignore_errors: false
      # Filter by parsed level field
      - type: level
        config:
          levels: ["INFO", "WARN", "ERROR"]
    config:
      addresses:
        - "http://elasticsearch:9200"
      index: "kafka-logs-{yyyy.MM.dd}"
      batch_size: 50
      timeout: 30
      # Resilient plugin configuration
      resilient: true
      retry_interval: 10
      max_retries: 0
      health_check_interval: 30
